#!/bin/bash

#SBATCH --job-name=mlip
#SBATCH --partition=gpusmall
#SBATCH --account=plantto # !!it is mandatory to specify the account (project) as precised in the CSC rules!!
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=10
#SBATCH --time=0-36:00:00
#SBATCH --gres=gpu:a100:1,nvme:10
#SBATCH --output=output_mlip_train_allegro_E-F-S_%j.txt
#SBATCH --error=errors_mlip_train_allegro_E-F-S_%j.txt

# Load necessary modules (!!checkh the availability of the following modules when using another machine!!)
# If certian modules are not available on your machine, contact the corrsponding admin(s) to install them.
module load pytorch/1.13 #necessary for 'nequip-train' and 'mir-allegro'

# using a virtual environment (!!when using v-env all the packages and dependencies should be installed in that v-env!!) 
# python -m venv myenv # should be used if there is no existent v-env
source /projappl/plantto/zakaryou/MLAMD/packages/allegro-nequip_test-install-venv/bin/activate

# Using the 'ulimit' command to control the user-level resource limits for processes. the option '-s unlimited' specifies that 
# there should be no limit on the stack size for the processes launched by the job. The stack size is the amount of memory 
# allocated for the call stack of a program.
ulimit -s unlimited

# Start of training the Allegro potential :
srun nequip-train 2xe_cc3_tba-config_new_vf.yaml
#--equivariance-test

exit 0
